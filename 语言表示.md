### 引言：NLP的核心关键——语言表示

​		词 是人类语言的构成基础。在处理文本语言时，将其以词为基本单位进行分割处理，是后续文本分析的基础。但两个词只要字面不同，就难以刻画它们之间的联系，比如“麦克风”和“话筒”（语义鸿沟现象），所以判断两个词是否相似，需要更多的背景知识

---



### 1. NLP中词的表示方式

#### 1.1 词的独热式表示 one-hot representation

​		这种方法是一种局部表示方法（Local Representation），把每个词表示为一个稀疏向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。举个栗子：

![image-20200723210145356](https://gitee.com/StanAugust/picbed/raw/master/img/20200729184833.png)

+ 优点：高维空间中，很多应用任务线性可分

+ 缺点：

  + 维数灾难
  
  + 所有的单词独占一个维度，单词间的依赖关系被忽略
  
  + 解决方案：
  
    将词“嵌入”低维空间，变为稠密向量，即**词嵌入(word embedding)**，这样词语就能在空间中关联起来
  

#### 1.2 词的分布式表示 distributed representation

​		是若干元素的连续表现形式，这种方法将词的语义**分散地存储在各个维度中**，又称**词嵌入**

<img src="https://gitee.com/StanAugust/picbed/raw/master/img/20200729185137.png" alt="20200725173118" style="zoom:80%;" />

##### 1.2.1 基于矩阵的分布式表示

​		基于矩阵的分布表示主要是构建”词-上下文“矩阵。

​		矩阵的行表示词，列表示上下文，每个元素表示某个词和上下文共现的次数（Count-based），这样矩阵的一行就描述了该词的上下文分布。

​		由于分布假说认为上下文相似的词，其语义也相似。因此这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

​		经典代表模型：$GloVe$，一种对“词-词”矩阵进行分解从而得到词表示的方法

​									https://github.com/stanfordnlp/GloVe

​									https://blog.csdn.net/coderTC/article/details/73864097

##### 1.2.2 基于神经网络的分布式表示

​		==语言模型（language model）：给定一个句子 $w$，语言模型就是计算句子的出现概率 $p(w)$ 的模型==

​		$Begino$等人在2003年正式提出了神经网络语言模型（ Neural Network Language Model ，$NNLM$​），该模型在学习语言模型的同时，也得到了词向量。因此在NLP语境中，**“词向量”特指由神经网络模型得到的低维实数向量表示。**

​		$Mikolov$等人在2013年开发出了$Word2Vec$，这是一个无监督学习的预训练嵌入**工具**，主要实现了两个语言模型：

+ 连续词袋模型（Continuous Bag-of-Words, $CBOW$）
+ Skip-Gram模型

​        这两种方式在算法上是相似的，差别在于$CBOW$从文本上下文单词中预测目标单词，而Skip-Gram则恰恰相反，它根据目标单词预测文本上下文单词。

​		具体推导过程百度

##### 1.2.3 基于聚类的分布式表示

​		不了解，暂且不表

